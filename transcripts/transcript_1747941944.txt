 I just came from Google IOT 2025 and my mind is honestly blown away. Hi everyone, I'm Shana Sharma. I'm currently in San Francisco recording this video right after the Google IOT event. There's so much to talk about. I'm going to share with you the talk 20 news from Google IOT 2025. These are going to help you learn how to become more productive, get things done faster and just be more efficient as a human being. So make sure that you was till the end. Hit the like button and subscribe and let's take a look at the top 20 craziest AI updates from Google IOT. The first one is VO3. VO3 is Google's latest state of the art video generation model in which you can not only generate videos which look ultra real with real world physics but can also generate audio. Just think about that. It creates background sounds, creates sound effects and also creates dialogue. They left behind a ball today. It bounced higher than I can jump. What manner of magic is that? This is honestly insane. There is no model right now cling AI or any other model which can create audio along with the video itself with a click of a button. You always have to use some other tool to do it and this is honestly incredible. Google IOT launch video was created with VO3 and it looked spectacular. It looked ultra real and I was just blown away by the quality of video that it was creating. Honestly to me this was the biggest AI update from Google IOT itself. The second update is image gen 4. Now image gen 4 is Google's latest image generation model. It is their reply to photo image generator by chat GPT and it's honestly pretty good with creating the right text, creating the right styles and just coming up with amazing pictures by just writing a simple text prompt. This was actually pretty incredible. I am really impressed by the generations that image gen 4 did. It looked very impressive in terms of the images it generated but the third update is actually what blew my mind and that is flow. Flow is a new application by Google which is going to help you create ultra realistic movies. Not just video using VO3 but create entire movie scenes and they generated videos, pictures in the video and created this entire storyline by just using flow. Flow is an incredible storytelling application that you can use which uses VO3 and image gen 4 to create amazing looking storyboards and visual storytelling and videos in general. It was incredible how much you can do with it. You can extend a particular scene, you can cut out a particular scene, you can always ask it to change certain elements and scenes and it does it seamlessly. I was honestly very impressed and I can't wait to try it out for creating better videos on YouTube for becoming a better storyteller. There's so much that you can do with flow if it actually turns out to be what they are saying. Update number is Liria 2 which is Google's own music generation model with which they actually showcased a video of Shankar Mahadevan creating parts of the music entirely by using AI and it was pretty impressive by the way he was able to do it. Just being a music composer not even having like actual knowledge of AI and the you know the research part of it and just being able to use it as a normal consumer that to me is incredible. The whole storyline for Google IO 25 was taking research into reality and that was pretty cool from which I see it. The next update is called Egentic Checkout and it's actually pretty cool. If you want to buy clothes but you're waiting for the next sale to come in Egentic Checkout by Google will actually blow your mind. So basically the way it works is it will notify you when the price drops. With the notification you can click on that notification and proceed to check out it automatically adds to the card selects your size by having personal context of you as a person and then proceeds to order it and you can just press a single button and pay it via cheapy. That is actually seamless it's incredible how easy it is for you to now purchase things and know exactly when the price drops. Related to this the sixth update is the shopping try-on feature. Now let's say you want to buy some clothes online but you're not sure if this will actually fit you well. Google now has a try-on feature wherein you can upload a picture of yourself a full-body image and give it the picture of the garment and it will then superimpose the garment on you and will do it with the utmost precision and you can see if this thing actually fits you well. It understands your body structure it understands the cloth the fitting of it and by using Gemini's multi-modality it creates this virtual try-on experience where you can see how you would look like when you wear this particular garment and that to me is honestly pretty crazy the way it is able to predict the fitting the sizing is honestly incredible and I can't be to try it out myself. Another insane AI news from Google is Android XR glasses. Yes you might have seen Google glasses and how spectacularly they failed but this one is honestly incredible. If you've heard about Meta-Raban glasses and how great they are at taking videos at helping you out with Meta-assistant now Google has just launched Android XR glasses and think of it like your virtual assistant which is with you on your glasses 24-7. Meaning Gemini can now see what you're seeing can give you instructions for any questions you might ask and also remembers everything. You can literally ask it that hey where did I keep my keys and it will actually know where did you keep your keys in your entire room in your entire house. It will also tell you directions for any place that you want to go and it will actually put it projected in front of your screen. So you can actually see where you have to go on the glasses itself. It's actually pretty incredible. They will be working with Bobby Parker and another company that I'm forgetting the name of to actually launch these first glasses and I can't wait to try it out. I am thinking that Meta AI would also be launching something very similar with the Rayband glasses so I'm looking out for that as well. I think this is going to be another consumer AI product that millions of people can actually start using as a normal consumer. The eighth update from Google is Google Beam in collaboration with HP. So Google has this amazing project called Project Starline where is think of it like a screen in which you can have a high fidelity meeting online with someone. Think of G-Mate and Zoom but much better. So you can actually see the shadows. You can see them actually popping out. It feels like you're actually sitting in front of them. If you're always tired of doing these virtual calls with people and you want like that in person experience, Google just announced this insane new update called Google Beam which is going to be a display with three different cameras plays at different locations which takes a 3D understanding of you, your face, your body and then generates a 60 hertz high fidelity version of you which looks very real for the other person. It's going to be a product, a screen which has three different cameras and can capture you and create a 3D version of you for the other person. It's honestly very incredible just like you can see on the screen and I'd love to try it out as well. The next update number nine is Google search AI mode. Just like you've had an AI overview in Google search, there's also an option of Google search AI mode which is powered by the latest Gemini 2.5 model which has deeper research capabilities which can browse through hundreds of websites to give you the most important one. It also has a personal context about you and it does it all seem less. So AI mode is basically Google's answer to complexity, pro search if you've tried it before and this is honestly pretty cool. Now there's also a deep research button that they have in which it will go and look up hundreds of websites and grab the most important data that you want and present it to you with just a single search. So Google search is going to be nothing like what you've seen before. It's going to be much more AI powered, much more relevant information and grounded information without hallucinations and that to me is pretty inculcable. The tenth update is Gemini agent mode. Now agents have always existed. There's a project meriner by Google that they are running but now let's say you're on the Gemini app. Let's say you want to check price of apartments nearby in your locality and you just have a simple search find apartments near my location under this budget and give me the best options. You press enter with all of your filters and then it will actually go on to Zillow or in Indian context 99 acres dot com or no broker dot com. As an agent it will go on these websites will browse through all of the properties nearby and depending on the filters you've added find the best properties for you and present it to you in the Gemini app itself. So this is the latest agent mode which is going to be out for everyone to try out and it's honestly pretty cool. It's just like operator but in the Gemini context. Now this is powered by project meriner which is actually the next update. Project meriner is now better than ever before. Project meriner basically for those of you who don't know is like Google's own version of operator. It's their agent app which can do anything on your behalf as an AI agent but this time it's getting a huge new update. It can now run 10 tasks at the same time. It also is available now on the Gemini API and on top of it it also has this very cool feature called teach and repeat. So imagine that you have a workflow that you create invoices for your customer for your client every day or you have a workflow of sending emails of creating canva designs. You can actually show it to project meriner teach it how you're doing it and then it would repeat it for you on your behalf and do it just the way you did it. And this is honestly very great for repetitive tasks that you normally have and just able to do it super quickly with the help of project meriner. Now update number 12 is project Astra and Gemini live. Now if you've ever used Google AI Studio you know there's an option in which you can show it what's in front of you using the camera of your phone and it will give you answers on what it can see. What is that part of the speaker called? That is the tweeter it produces high frequency sounds. You can ask anything I can ask you to name this this laptop I can ask you to help me out fix my bicycle or use a microwave oven or just anything that I want to use. I can even share it with my screen and ask you to learn how to use a particular app like Excel or any other tool. Now with the project Astra coming into Gemini live you can now show it anything around you and ask and get much more deeper personal contextualized answers. Basically you have those Astra glasses but the same technology is now available on the Gemini live itself. You can now show it anything around you and get ultra contextual answers. What can I add here to make this system faster? Adding a cache between the server and database could improve speed. What does this remind you of? Trudginger's Cat. There was this very funny video in which there was a lady who was looking at a garbage truck and saying that's a nice convertible and it would reply saying that that's not a convertible that's actually a garbage truck. That's a pretty nice convertible. I think you might have mistaken the garbage truck for a convertible. Is there anything else I can help you with? What's this skinny building during my neighborhood? It's a street light, not a building. It literally understands what's around you. It also results with you and tells you what's the right thing and that is pretty cool. Now update number 13 is Gemini's latest model family which is Gemini 2.5. Now you've already seen 2.5-throw drop and it's the best model out there according to LM Arena to build anything that you want to. But now you have 2.5 flash, 2.5 flash light, the new models which are much faster with similar capacity of running AI tasks. But now they also just dropped Gemini 2.5 Pro Deep Thinking, which is their best in class deep thinking reasoning model. Now with this you can essentially do anything you want in math, in coding, in multi-modality and it does it with utmost efficiency and that to me is pretty crazy. Now Gemini's 2.5 model is already the best in class. They've just launched 2.5 Pro Deep Thinking to make it even better. So this is something that you should definitely try out. You can go and try it out on Google AI Studio or on what XAI or any other tool that Google is currently offering maybe on the Gemini app as well. The 14th update is something called Synth ID which is their invisible watermark that they have set for anything AI generated that Gemini or Google creates. So any video you make with VO3, any image you create with image and any storytelling video you make with flow or any other form of media that you create, it has this invisible watermark that Google has put up to track how many people have generated how many different AI generated media. And there's actually 10 billion pieces of content today on the internet which is AI generated which has that Synth ID which to me is honestly insane. Just think about that. Just people creating AI videos and pictures using Gemini, 10 billion in total. That's honestly insane. The next update is Gemini Text diffusion which is pretty cool because normally you would ask Gemini to solve a math question or generate some code and perhaps you write it out itself. It has a text completion model. That's what LLMs are supposed to do. But now they've used diffusion which they normally use for creating images to write text. And it's apparently much faster for you to use Gemini Text diffusion to write code to solve math problems and do it all with ease by just using diffusion. That was pretty cool for me to see. I did not expect that you can apply something for creating pictures, diffusion to now creating text as well. That was pretty cool. The 16th update from Google IOS Stitch. If you don't know anything about design or how to code apps but you still want to build apps from scratch, Google has launched an incredible new AI tool which will blow your mind. Stitch is excellent app which not only creates code but also creates amazing looking designs. So give it a prompt that I want to create a simple app, press enter but then show you the prototypes, the wireframes of the design of the app that you can expect. You then approve those designs and then it writes the code for it. These are not screenshots. These are actual designs like Figma designs with each element having a different ID and you can change stuff if you want to. And then it turns those designs into ready to use code and ready to use websites. And then you can even deploy it using Google itself. That to me is actually pretty crazy. Now imagine a world in which you can go from text to design to code to a deployed app that millions of people can use. And that's the promise of Stitch and this app by Google. Try it out yourself and let me know what you think of it. Google Labs has just these amazing tools that you can be trying out today. Now the 17th update is Jules Coding Agent which is their own version of GitHub Copilot or Replic. The way it works is that it can create entire code basis for you. You can even drop in your own code base and understands all the context and it helps you go from text to ready to use apps in minutes. And that is pretty cool. Firebase Studio does something very similar and I just love how anyone today can take up anything that they want to build and just build it by just writing text. That's all that you need. The 18th update from Google I.O is Gemini in Chrome. Think of a browsing agent which can assist you for any question you have about any website that you're visiting. It's now possible with Gemini in Chrome. It is now a button that you can press and you can ask any question about what you are seeing on your screen. You can ask it to schedule something. You can ask it to do anything else and because it has access to your entire Google suite, you can do it effortlessly by just tapping on Gemini in Chrome. That was actually very powerful and very handy in general. The 19th update is super cool. It's called Google Meet Life Translation. Let's say I'm talking in Hindi to you and you only understand let's say Mandarin. Now Google Meet will now understand that Hindi is being spoken. Convoted into English so that I can read it and understand what this person is saying. And then the other person is speaking Mandarin. It will turn into English for me to understand what the next person is speaking. Hi Camilla. Let me turn on speech translation. It's nice to finally talk to you. It's good to speak in the end. You're going to have a lot of fun and I think you're going to love visiting the city. The house is in a very nice neighborhood and overlooks the mountains. So now two people who don't have any common language, only understand English can communicate with each other in real time on a G-METE and that is actually a pretty crazy update. Now let me tell you the coolest one and this is Google AI pricing. If you thought that they're going to give all of this value for you for free to use, it's actually true. Like a lot of the tools you can actually start using for free but they have limits and you can actually get Google's plans to get more limits to get access to their latest plans and models like VO3 and image in 4 and flow and other ones and it's going to cost you money. Their first plan is $12 a month which gives you access to VO2, gives you access to flow, gives you access to other AI tools and the next one is actually Google AI Ultra. Now Google has unveiled the most expensive plan to get access to AI tools that Google creates. It's called Google AI Ultra. For $250 a month, you get access to VO3, you get access to image in 4, you get access to flow, you get access to all of their agentic capabilities and more. That to me is insane. Companies are now readily charging $2 a month. Would you pay something like that to get access to all of these AI tools? And if so, what value do you derive from it? Let me know below in the comments. I'd love to hear your responses. This was the ultimate summary of Google IO 2025. I had so much fun. The highlight that I can understand from this entire event is that Google is coming for all the AI tools out there. They are going to do their best job to defeat all tools and give you the best experience to use everything in the Google suite itself and it's going to be very exciting to see how that unfolds. Thank you so much for watching this video. I hope you enjoyed it. Hit like button and subscribe. Let me know in the comments if you have any questions. I'll be coming back to India to shoot more content. I have some podcasts to record. There's something amazing cooking up with OpenAI. Stay tuned for that. I'll see you in the next one. Bye bye.